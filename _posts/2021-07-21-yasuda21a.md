---
title: "Exponentially Improved Dimensionality Reduction for\r l1: Subspace Embeddings
  and Independence Testing"
abstract: "Despite many applications, dimensionality reduction\r in the $\\ell_1$-norm
  is much less understood than in\r the Euclidean norm. We give two new oblivious\r
  dimensionality reduction techniques for the\r $\\ell_1$-norm which improve {\\it
  exponentially} over\r prior ones: \\begin{enumerate} \\item We design a\r distribution
  over random matrices $S \\in\r \\mathbb{R}^{r \\times n}$, where $r =\r 2^{\\textrm{poly}(d/(\\epsilon
  \\delta))}$, such that\r given any matrix $A \\in \\mathbb{R}^{n \\times d}$,\r
  with probability at least $1-\\delta$, simultaneously\r for all $x$, $\\|SAx\\|_1
  = (1 \\pm\r \\epsilon)\\|Ax\\|_1$. Note that $S$ is linear, does\r not depend on
  $A$, and maps $\\ell_1$ into\r $\\ell_1$. Our distribution provides an exponential\r
  improvement on the previous best known map of Wang\r and Woodruff (SODA, 2019),
  which required $r =\r 2^{2^{\\Omega(d)}}$, even for constant $\\epsilon$ and\r $\\delta$.
  Our bound is optimal, up to a polynomial\r factor in the exponent, given a known\r
  $2^{\\textrm{poly}(d)}$ lower bound for constant\r $\\epsilon$ and $\\delta$.  \\item
  We design a\r distribution over matrices $S \\in \\mathbb{R}^{k\r \\times n}$, where
  $k = 2^{O(q^2)}(\\epsilon^{-1} q\r \\log d)^{O(q)}$, such that given any $q$-mode
  tensor\r $A \\in (\\mathbb{R}^{d})^{\\otimes q}$, one can\r estimate the entrywise
  $\\ell_1$-norm $\\|A\\|_1$ from\r $S(A)$. Moreover, $S = S^1 \\otimes S^2 \\otimes\r
  \\cdots \\otimes S^q$ and so given vectors $u_1,\r \\ldots, u_q \\in \\mathbb{R}^d$,
  one can compute\r $S(u_1 \\otimes u_2 \\otimes \\cdots \\otimes u_q)$ in\r time
  $2^{O(q^2)}(\\epsilon^{-1} q \\log d)^{O(q)}$,\r which is much faster than the $d^q$
  time required to\r form $u_1 \\otimes u_2 \\otimes \\cdots \\otimes\r u_q$. Our
  linear map gives a streaming algorithm for\r independence testing using space\r
  $2^{O(q^2)}(\\epsilon^{-1} q \\log d)^{O(q)}$,\r improving the previous doubly exponential\r
  $(\\epsilon^{-1} \\log d)^{q^{O(q)}}$ space bound of\r Braverman and Ostrovsky (STOC,
  2010).\r \\end{enumerate} For subspace embeddings, we also\r study the setting when
  $A$ is itself drawn from\r distributions with independent entries, and obtain a\r
  polynomial embedding dimension. For independence\r testing, we also give algorithms
  for any distance\r measure with a polylogarithmic-sized sketch and\r satisfying
  an approximate triangle inequality."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yasuda21a
month: 0
tex_title: "Exponentially Improved Dimensionality Reduction for\r l1: Subspace Embeddings
  and Independence Testing"
firstpage: 3111
lastpage: 3195
page: 3111-3195
order: 3111
cycles: false
bibtex_author: Yasuda, Taisuke and Woodruff, David and Li, Yi
author:
- given: Taisuke
  family: Yasuda
- given: David
  family: Woodruff
- given: Yi
  family: Li
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/yasuda21a/yasuda21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
