---
title: 'Exponentially Improved Dimensionality Reduction for l1: Subspace Embeddings
  and Independence Testing'
abstract: 'Despite many applications, dimensionality reduction in the $\ell_1$-norm
  is much less understood than in the Euclidean norm. We give two new oblivious dimensionality
  reduction techniques for the $\ell_1$-norm which improve {\it exponentially} over
  prior ones: \begin{enumerate} \item We design a distribution over random matrices
  $S \in \mathbb{R}^{r \times n}$, where $r = 2^{\textrm{poly}(d/(\epsilon \delta))}$,
  such that given any matrix $A \in \mathbb{R}^{n \times d}$, with probability at
  least $1-\delta$, simultaneously for all $x$, $\|SAx\|_1 = (1 \pm \epsilon)\|Ax\|_1$.
  Note that $S$ is linear, does not depend on $A$, and maps $\ell_1$ into $\ell_1$.
  Our distribution provides an exponential improvement on the previous best known
  map of Wang and Woodruff (SODA, 2019), which required $r = 2^{2^{\Omega(d)}}$, even
  for constant $\epsilon$ and $\delta$. Our bound is optimal, up to a polynomial factor
  in the exponent, given a known $2^{\textrm{poly}(d)}$ lower bound for constant $\epsilon$
  and $\delta$.  \item We design a distribution over matrices $S \in \mathbb{R}^{k
  \times n}$, where $k = 2^{O(q^2)}(\epsilon^{-1} q \log d)^{O(q)}$, such that given
  any $q$-mode tensor $A \in (\mathbb{R}^{d})^{\otimes q}$, one can estimate the entrywise
  $\ell_1$-norm $\|A\|_1$ from $S(A)$. Moreover, $S = S^1 \otimes S^2 \otimes \cdots
  \otimes S^q$ and so given vectors $u_1, \ldots, u_q \in \mathbb{R}^d$, one can compute
  $S(u_1 \otimes u_2 \otimes \cdots \otimes u_q)$ in time $2^{O(q^2)}(\epsilon^{-1}
  q \log d)^{O(q)}$, which is much faster than the $d^q$ time required to form $u_1
  \otimes u_2 \otimes \cdots \otimes u_q$. Our linear map gives a streaming algorithm
  for independence testing using space $2^{O(q^2)}(\epsilon^{-1} q \log d)^{O(q)}$,
  improving the previous doubly exponential $(\epsilon^{-1} \log d)^{q^{O(q)}}$ space
  bound of Braverman and Ostrovsky (STOC, 2010). \end{enumerate} For subspace embeddings,
  we also study the setting when $A$ is itself drawn from distributions with independent
  entries, and obtain a polynomial embedding dimension. For independence testing,
  we also give algorithms for any distance measure with a polylogarithmic-sized sketch
  and satisfying an approximate triangle inequality.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yasuda21a
month: 0
tex_title: 'Exponentially Improved Dimensionality Reduction for l1: Subspace Embeddings
  and Independence Testing'
firstpage: 3111
lastpage: 3195
page: 3111-3195
order: 3111
cycles: false
bibtex_author: Yasuda, Taisuke and Woodruff, David and Li, Yi
author:
- given: Taisuke
  family: Yasuda
- given: David
  family: Woodruff
- given: Yi
  family: Li
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/yasuda21a/yasuda21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
