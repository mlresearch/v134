---
title: 'Modeling from Features: a Mean-field Framework for Over-parameterized Deep
  Neural Networks'
abstract: This paper proposes a new mean-field framework for over-parameterized deep
  neural networks (DNNs), which can be used to analyze neural network training.  In
  this framework, a DNN is represented by probability measures and functions over
  its features (that is, the function values of the hidden units over the training
  data) in the continuous limit, instead of the neural network parameters as most
  existing studies have done. This new representation overcomes the degenerate situation
  where all the hidden units essentially have only one meaningful hidden unit in each
  middle layer, leading to a simpler representation of DNNs. Moreover, we construct
  a non-linear dynamics called neural feature flow, which captures the evolution of
  an over-parameterized DNN trained by Gradient Descent. We illustrate the framework
  via the Residual Network (Res-Net) architecture.  It is shown that when the neural
  feature flow process converges, it reaches a global minimal solution under suitable
  conditions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fang21a
month: 0
tex_title: 'Modeling from Features: a Mean-field Framework for Over-parameterized
  Deep Neural Networks'
firstpage: 1887
lastpage: 1936
page: 1887-1936
order: 1887
cycles: false
bibtex_author: Fang, Cong and Lee, Jason and Yang, Pengkun and Zhang, Tong
author:
- given: Cong
  family: Fang
- given: Jason
  family: Lee
- given: Pengkun
  family: Yang
- given: Tong
  family: Zhang
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/fang21a/fang21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
