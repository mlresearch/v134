---
title: "Implicit Regularization in ReLU Networks with the\r Square Loss"
abstract: "Understanding the implicit regularization (or\r implicit bias) of gradient
  descent has recently been\r a very active research area. However, the implicit\r
  regularization in nonlinear neural networks is still\r poorly understood, especially
  for regression losses\r such as the square loss. Perhaps surprisingly, we\r prove
  that even for a single ReLU neuron, it is\r impossible to characterize the implicit\r
  regularization with the square loss by any explicit\r function of the model parameters
  (although on the\r positive side, we show it can be characterized\r approximately).
  For one hidden-layer networks, we\r prove a similar result, where in general it
  is\r impossible to characterize implicit regularization\r properties in this manner,
  except for the\r “balancedness” property identified in Du et\r al. (2018). Our results
  suggest that a more general\r framework than the one considered so far may be\r
  needed to understand implicit regularization for\r nonlinear predictors, and provides
  some clues on\r what this framework should be."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vardi21b
month: 0
tex_title: "Implicit Regularization in ReLU Networks with the\r Square Loss"
firstpage: 4224
lastpage: 4258
page: 4224-4258
order: 4224
cycles: false
bibtex_author: Vardi, Gal and Shamir, Ohad
author:
- given: Gal
  family: Vardi
- given: Ohad
  family: Shamir
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/vardi21b/vardi21b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
