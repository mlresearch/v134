---
title: Implicit Regularization in ReLU Networks with the Square Loss
abstract: Understanding the implicit regularization (or implicit bias) of gradient
  descent has recently been a very active research area. However, the implicit regularization
  in nonlinear neural networks is still poorly understood, especially for regression
  losses such as the square loss. Perhaps surprisingly, we prove that even for a single
  ReLU neuron, it is impossible to characterize the implicit regularization with the
  square loss by any explicit function of the model parameters (although on the positive
  side, we show it can be characterized approximately). For one hidden-layer networks,
  we prove a similar result, where in general it is impossible to characterize implicit
  regularization properties in this manner, except for the “balancedness” property
  identified in Du et al. (2018). Our results suggest that a more general framework
  than the one considered so far may be needed to understand implicit regularization
  for nonlinear predictors, and provides some clues on what this framework should
  be.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vardi21b
month: 0
tex_title: Implicit Regularization in ReLU Networks with the Square Loss
firstpage: 4224
lastpage: 4258
page: 4224-4258
order: 4224
cycles: false
bibtex_author: Vardi, Gal and Shamir, Ohad
author:
- given: Gal
  family: Vardi
- given: Ohad
  family: Shamir
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/vardi21b/vardi21b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
