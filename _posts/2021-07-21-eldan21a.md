---
title: Non-asymptotic approximations of neural networks by Gaussian processes
abstract: We study the extent to which wide neural networks may be approximated by
  Gaussian processes, when initialized with random weights. It is a well-established
  fact that as the width of a network goes to infinity, its law converges to that
  of a Gaussian process. We make this quantitative by establishing explicit convergence
  rates for the central limit theorem in an infinite-dimensional functional space,
  metrized with a natural transportation distance. We identify two regimes of interest;
  when the activation function is polynomial, its degree determines the rate of convergence,
  while for non-polynomial activations, the rate is governed by the smoothness of
  the function.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eldan21a
month: 0
tex_title: Non-asymptotic approximations of neural networks by Gaussian processes
firstpage: 1754
lastpage: 1775
page: 1754-1775
order: 1754
cycles: false
bibtex_author: Eldan, Ronen and Mikulincer, Dan and Schramm, Tselil
author:
- given: Ronen
  family: Eldan
- given: Dan
  family: Mikulincer
- given: Tselil
  family: Schramm
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/eldan21a/eldan21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
