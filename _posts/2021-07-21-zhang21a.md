---
title: Learning to Stop with Surprisingly Few Samples
abstract: 'We consider a discounted infinite horizon optimal stopping problem. If
  the underlying distribution is known a priori, the solution of this problem is obtained
  via dynamic programming (DP) and is given by a well known threshold rule. When information
  on this distribution is lacking, a natural (though naive) approach is “explore-then-exploit,"
  whereby the unknown distribution or its parameters are estimated over an initial
  exploration phase, and this estimate is then used in the DP to determine actions
  over the residual exploitation phase. We show: (i) with proper tuning, this approach
  leads to performance comparable to the full information DP solution; and (ii) despite
  common wisdom on the sensitivity of such “plug in" approaches in DP due to propagation
  of estimation errors, a surprisingly “short" (logarithmic in the horizon) exploration
  horizon suffices to obtain said performance.  In cases where the underlying distribution
  is heavy-tailed, these observations are even more pronounced: a single sample exploration
  phase suffices.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21a
month: 0
tex_title: Learning to Stop with Surprisingly Few Samples
firstpage: 3887
lastpage: 3888
page: 3887-3888
order: 3887
cycles: false
bibtex_author: Zhang, Tianyi and Russo, Daniel and Zeevi, Assaf
author:
- given: Tianyi
  family: Zhang
- given: Daniel
  family: Russo
- given: Assaf
  family: Zeevi
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/zhang21a/zhang21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
