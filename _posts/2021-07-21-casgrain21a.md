---
title: 'Optimizing Optimizers: Regret-optimal gradient descent algorithms'
abstract: This paper treats the task of designing optimization algorithms as an optimal
  control problem. Using regret as a metric for an algorithmâ€™s performance, we study
  the existence, uniqueness and consistency of regret-optimal algorithms. By providing
  first-order optimality conditions for the control problem, we show that regret-optimal
  algorithms must satisfy a specific structure in their dynamics which we show is
  equivalent to performing \emph{dual-preconditioned gradient descent} on the value
  function generated by its regret. Using these optimal dynamics, we provide bounds
  on their rates of convergence to solutions of convex optimization problems. Though
  closed-form optimal dynamics cannot be obtained in general, we present fast numerical
  methods for approximating them, generating optimization algorithms which directly
  optimize their long-term regret. These are benchmarked against commonly used optimization
  algorithms to demonstrate their effectiveness.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: casgrain21a
month: 0
tex_title: 'Optimizing Optimizers: Regret-optimal gradient descent algorithms'
firstpage: 883
lastpage: 926
page: 883-926
order: 883
cycles: false
bibtex_author: Casgrain, Philippe and Kratsios, Anastasis
author:
- given: Philippe
  family: Casgrain
- given: Anastasis
  family: Kratsios
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/casgrain21a/casgrain21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
