---
title: "SGD in the Large: Average-case Analysis,\r Asymptotics, and Stepsize Criticality"
abstract: "We propose a new framework, inspired by random\r matrix theory, for analyzing
  the dynamics of\r stochastic gradient descent (SGD) when both number\r of samples
  and dimensions are large. This framework\r applies to any fixed stepsize and on
  the least\r squares problem with random data (finite-sum). Using\r this new framework,
  we show that the dynamics of SGD\r become deterministic in the large sample and\r
  dimensional limit. Furthermore, the limiting\r dynamics are governed by a Volterra
  integral\r equation. This model predicts that SGD undergoes a\r phase transition
  at an explicitly given critical\r stepsize that ultimately affects its convergence\r
  rate, which is also verified\r experimentally. Finally, when input data is\r isotropic,
  we provide explicit expressions for the\r dynamics and average-case convergence
  rates (i.e.,\r the complexity of an algorithm averaged over all\r possible inputs).
  These rates show significant\r improvement over classical worst-case complexities."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: paquette21a
month: 0
tex_title: "SGD in the Large: Average-case Analysis,\r Asymptotics, and Stepsize Criticality"
firstpage: 3548
lastpage: 3626
page: 3548-3626
order: 3548
cycles: false
bibtex_author: Paquette, Courtney and Lee, Kiwon and Pedregosa, Fabian and Paquette,
  Elliot
author:
- given: Courtney
  family: Paquette
- given: Kiwon
  family: Lee
- given: Fabian
  family: Pedregosa
- given: Elliot
  family: Paquette
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/paquette21a/paquette21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
