---
title: 'Shape Matters: Understanding the Implicit Bias of the Noise Covariance'
abstract: The noise in stochastic gradient descent (SGD) provides a crucial implicit
  regularization effect for training overparameterized models. Prior theoretical work
  largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate
  the phenomenon that parameter-dependent noise — induced by mini-batches or label
  perturbation — is far more effective than Gaussian noise.  This paper theoretically
  characterizes this phenomenon on a quadratically-parameterized model introduced
  by Vaskevicius et al. and Woodworth et al.  We show that in an over-parameterized
  setting, SGD with label noise recovers the sparse ground-truth with an arbitrary
  initialization, whereas SGD with Gaussian noise or gradient descent overfits to
  dense solutions with large norms. Our analysis reveals that parameter-dependent
  noise introduces a bias towards local minima with smaller noise variance, whereas
  spherical Gaussian noise does not.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: haochen21a
month: 0
tex_title: 'Shape Matters: Understanding the Implicit Bias of the Noise Covariance'
firstpage: 2315
lastpage: 2357
page: 2315-2357
order: 2315
cycles: false
bibtex_author: HaoChen, Jeff Z. and Wei, Colin and Lee, Jason and Ma, Tengyu
author:
- given: Jeff Z.
  family: HaoChen
- given: Colin
  family: Wei
- given: Jason
  family: Lee
- given: Tengyu
  family: Ma
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/haochen21a/haochen21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
