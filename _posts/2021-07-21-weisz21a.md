---
title: "On Query-efficient Planning in MDPs under Linear\r Realizability of the Optimal
  State-value Function"
abstract: "We consider the problem of local planning in\r fixed-horizon Markov Decision
  Processes (MDPs) with\r a generative model under the assumption that the\r optimal
  value function lies close to the span of a\r feature map. The generative model provides
  a\r restricted, “local” access to the MDP: The planner\r can ask for random transitions
  from previously\r returned states and arbitrary actions, and the\r features are
  also only accessible for the states\r that are encountered in this process. As opposed
  to\r previous work (e.g. Lattimore et al. (2020)) where\r linear realizability of
  all policies was assumed, we\r consider the significantly relaxed assumption of
  a\r single linearly realizable (deterministic) policy. A\r recent lower bound by
  Weisz et al. (2020)\r established that the related problem when the\r action-value
  function of the optimal policy is\r linearly realizable requires an exponential
  number\r of queries, either in $H$ (the horizon of the MDP)\r or $d$ (the dimension
  of the feature mapping). Their\r construction crucially relies on having an\r exponentially
  large action set. In contrast, in this\r work, we establish that $\\poly(H,d)$ planning
  is\r possible with state value function realizability\r whenever the action set
  has a constant size. In\r particular, we present the TensorPlan algorithm\r which
  uses $\\poly((dH/\\delta)^A)$ simulator queries\r to find a $\\delta$-optimal policy
  relative to any\r deterministic policy for which the value function is\r linearly
  realizable with some bounded parameter\r (with a known bound). This is the first
  algorithm to\r give a polynomial query complexity guarantee using\r only linear-realizability
  of a single competing\r value function. Whether the computation cost is\r similarly
  bounded remains an interesting open\r question. We also extend the upper bound to
  the\r near-realizable case and to the infinite-horizon\r discounted MDP setup. The
  upper bounds are\r complemented by a lower bound which states that in\r the infinite-horizon
  episodic setting, planners that\r achieve constant suboptimality need exponentially\r
  many queries, either in the dimension or the number\r of actions."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: weisz21a
month: 0
tex_title: "On Query-efficient Planning in MDPs under Linear\r Realizability of the
  Optimal State-value Function"
firstpage: 4355
lastpage: 4385
page: 4355-4385
order: 4355
cycles: false
bibtex_author: Weisz, Gellert and Amortila, Philip and Janzer, Barnab\'as and Abbasi-Yadkori,
  Yasin and Jiang, Nan and Szepesvari, Csaba
author:
- given: Gellert
  family: Weisz
- given: Philip
  family: Amortila
- given: Barnabás
  family: Janzer
- given: Yasin
  family: Abbasi-Yadkori
- given: Nan
  family: Jiang
- given: Csaba
  family: Szepesvari
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/weisz21a/weisz21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
