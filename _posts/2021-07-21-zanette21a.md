---
title: Cautiously Optimistic Policy Optimization and Exploration with Linear Function
  Approximation
abstract: Policy optimization methods are popular reinforcement learning (RL) algorithms,
  because their incremental and on-policy nature makes them more stable than the value-based
  counterparts. However, the same properties also make them slow to converge and sample
  inefficient, as the on-policy nature is not amenable to data reuse and the incremental
  updates result in a large iteration complexity before a near optimal policy is discovered.
  These empirical findings are mirrored in theory in the recent work ofÂ \citet{agarwal2020pc},
  which provides a policy optimization method PC-PG that provably finds near optimal
  policies in the linear MDP model and is robust to model misspecification, but suffers
  from an extremely poor sample complexity compared with value-based techniques.  In
  this paper, we propose a new algorithm, COPOE, that overcomes the poor sample complexity
  of PC-PG while retaining the robustness to model misspecification.  COPOE makes
  several important algorithmic enhancements of PC-PG, such as enabling data reuse,
  and uses more refined analysis techniques, which we expect to be more broadly applicable
  to designing new RL algorithms.  The result is an improvement in sample complexity
  from $\widetilde{O}(1/\epsilon^{11})$ for PC-PG to $\widetilde{O}(1/\epsilon^3)$
  for COPOE, nearly bridging the gap with value-based techniques.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zanette21a
month: 0
tex_title: Cautiously Optimistic Policy Optimization and Exploration with Linear Function
  Approximation
firstpage: 4473
lastpage: 4525
page: 4473-4525
order: 4473
cycles: false
bibtex_author: Zanette, Andrea and Cheng, Ching-An and Agarwal, Alekh
author:
- given: Andrea
  family: Zanette
- given: Ching-An
  family: Cheng
- given: Alekh
  family: Agarwal
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/zanette21a/zanette21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
