---
title: "Information-Theoretic Generalization Bounds for\r Stochastic Gradient Descent"
abstract: "We study the generalization properties of the\r popular stochastic optimization
  method known as\r stochastic gradient descent (SGD) for optimizing\r general non-convex
  loss functions. Our main\r contribution is providing upper bounds on the\r generalization
  error that depend on local statistics\r of the stochastic gradients evaluated along
  the path\r of iterates calculated by SGD. The key factors our\r bounds depend on
  are the variance of the gradients\r (with respect to the data istribution) and the
  local\r smoothness of the objective function along the SGD\r path, and the sensitivity
  of the loss function to\r perturbations to the final output. Our key technical\r
  tool is combining the information-theoretic\r generalization bounds previously used
  for analyzing\r randomized variants of SGD with a perturbation\r analysis of the
  iterates."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: neu21a
month: 0
tex_title: "Information-Theoretic Generalization Bounds for\r Stochastic Gradient
  Descent"
firstpage: 3526
lastpage: 3545
page: 3526-3545
order: 3526
cycles: false
bibtex_author: Neu, Gergely
author:
- given: Gergely
  family: Neu
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/neu21a/neu21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
