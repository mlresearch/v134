---
title: Information-Theoretic Generalization Bounds for Stochastic Gradient Descent
abstract: We study the generalization properties of the popular stochastic optimization method known as  stochastic gradient descent (SGD) for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: neu21a
month: 0
tex_title: Information-Theoretic Generalization Bounds for Stochastic Gradient Descent
firstpage: 3526
lastpage: 3545
page: 3526-3545
order: 3526
cycles: false
bibtex_author: Neu, Gergely and Dziugaite, Gintare Karolina and Haghifam, Mahdi and Roy, Daniel M.
author:
- given: Gergely
  family: Neu
- given: Gintare Karolina
  family: Dziugaite
- given: Mahdi
  family: Haghifam
- given: Daniel M.
  family: Roy
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/neu21a/neu21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
