---
title: SGD Generalizes Better Than GD (And Regularization Doesn’t Help)
abstract: We give a new separation result between the generalization performance of
  stochastic gradient descent (SGD) and of full-batch gradient descent (GD) in the
  fundamental stochastic convex optimization model. While for SGD it is well-known
  that $O(1/\epsilon^2)$ iterations suffice for obtaining a solution with $\epsilon$
  excess expected risk, we show that with the same number of steps GD may overfit
  and emit a solution with $\Omega(1)$ generalization error. Moreover, we show that
  in fact $\Omega(1/\epsilon^4)$ iterations are necessary for GD to match the generalization
  performance of SGD, which is also tight due to recent work by Bassily et al. (2020).
  We further discuss how regularizing the empirical risk minimized by GD essentially
  does not change the above result, and revisit the concepts of stability, implicit
  bias and the role of the learning algorithm in generalization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: amir21a
month: 0
tex_title: SGD Generalizes Better Than GD (And Regularization Doesn’t Help)
firstpage: 63
lastpage: 92
page: 63-92
order: 63
cycles: false
bibtex_author: Amir, Idan and Koren, Tomer and Livni, Roi
author:
- given: Idan
  family: Amir
- given: Tomer
  family: Koren
- given: Roi
  family: Livni
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/amir21a/amir21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
