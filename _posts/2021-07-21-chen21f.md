---
title: "Impossible Tuning Made Possible: A New Expert\r Algorithm and Its Applications"
abstract: "We resolve the long-standing “impossible tuning”\r issue for the classic
  expert problem and show that,\r it is in fact possible to achieve regret\r $O\\Big(\\sqrt{(\\ln
  d)\\sumt \\ell_{t,i}^2}\\Big)$\r simultaneously for all expert $i$ in a $T$-round\r
  $d$-expert problem where $\\ell_{t,i}$ is the loss\r for expert $i$ in round $t$.
  \ Our algorithm is based\r on the Mirror Descent framework with a correction\r term
  and a weighted entropy regularizer.  While\r natural, the algorithm has not been
  studied before\r and requires a careful analysis.  We also generalize\r the bound
  to $O\\Big(\\sqrt{(\\ln d)\\sumt\r (\\ell_{t,i}-m_{t,i})^2}\\Big)$ for any prediction\r
  vector $m_t$ that the learner receives, and recover\r or improve many existing results
  by choosing\r different $m_t$.  Furthermore, we use the same\r framework to create
  a master algorithm that combines\r a set of base algorithms and learns the best
  one\r with little overhead.  The new guarantee of our\r master allows us to derive
  many new results for both\r the expert problem and more generally Online Linear\r
  Optimization."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen21f
month: 0
tex_title: "Impossible Tuning Made Possible: A New Expert\r Algorithm and Its Applications"
firstpage: 1216
lastpage: 1259
page: 1216-1259
order: 1216
cycles: false
bibtex_author: Chen, Liyu and Luo, Haipeng and Wei, Chen-Yu
author:
- given: Liyu
  family: Chen
- given: Haipeng
  family: Luo
- given: Chen-Yu
  family: Wei
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/chen21f/chen21f.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
