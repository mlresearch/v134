---
title: "Non-stationary Reinforcement Learning without Prior\r Knowledge: an Optimal
  Black-box Approach"
abstract: "We propose a black-box reduction that turns a\r certain reinforcement learning
  algorithm with\r optimal regret in a (near-)stationary environment\r into another
  algorithm with optimal dynamic regret\r in a non-stationary environment, importantly
  without\r any prior knowledge on the degree of\r non-stationarity.  By plugging
  different algorithms\r into our black-box, we provide a list of examples\r showing
  that our approach not only recovers recent\r results for (contextual) multi-armed
  bandits\r achieved by very specialized algorithms, but also\r significantly improves
  the state of the art for\r (generalzed) linear bandits, episodic MDPs, and\r infinite-horizon
  MDPs in various ways.\r Specifically, in most cases our algorithm achieves\r the
  optimal dynamic regret\r $\\widetilde{\\mathcal{O}}(\\min\\{\\sqrt{LT},\r \\Delta^{\\frac{1}{3}}T^{\\frac{2}{3}}\\})$
  where $T$ is\r the number of rounds and $L$ and $\\Delta$ are the\r number and amount
  of changes of the world\r respectively, while previous works only obtain\r suboptimal
  bounds and/or require the knowledge of\r $L$ and $\\Delta$."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wei21b
month: 0
tex_title: "Non-stationary Reinforcement Learning without Prior\r Knowledge: an Optimal
  Black-box Approach"
firstpage: 4300
lastpage: 4354
page: 4300-4354
order: 4300
cycles: false
bibtex_author: Wei, Chen-Yu and Luo, Haipeng
author:
- given: Chen-Yu
  family: Wei
- given: Haipeng
  family: Luo
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/wei21b/wei21b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
