---
title: Robust Online Convex Optimization in the Presence of Outliers
abstract: We consider online convex optimization when a number k of data points are
  outliers that may be corrupted. We model this by introducing the notion of robust
  regret, which measures the regret only on rounds that are not outliers. The aim
  for the learner is to achieve small robust regret, without knowing where the outliers
  are. If the outliers are chosen adversarially, we show that a simple filtering strategy
  on extreme gradients incurs O(k) overhead compared to the usual regret bounds, and
  that this is unimprovable, which means that k needs to be sublinear in the number
  of rounds. We further ask which additional assumptions would allow for a linear
  number of outliers. It turns out that the usual benign cases of independently, identically
  distributed (i.i.d.) observations or strongly convex losses are not sufficient.
  However, combining i.i.d. observations with the assumption that outliers are those
  observations that are in an extreme quantile of the distribution, does lead to sublinear
  robust regret, even though the expected number of outliers is linear.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vanerven21a
month: 0
tex_title: Robust Online Convex Optimization in the Presence of Outliers
firstpage: 4174
lastpage: 4194
page: 4174-4194
order: 4174
cycles: false
bibtex_author: van Erven, Tim and Sachs, Sarah and Koolen, Wouter M and Kotlowski,
  Wojciech
author:
- given: Tim
  family: van Erven
- given: Sarah
  family: Sachs
- given: Wouter M
  family: Koolen
- given: Wojciech
  family: Kotlowski
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/vanerven21a/vanerven21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
