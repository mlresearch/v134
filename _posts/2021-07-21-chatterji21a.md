---
title: "When does gradient descent with logistic loss\r interpolate using deep networks
  with smoothed ReLU\r activations?"
abstract: "We establish conditions under which gradient descent\r applied to fixed-width
  deep networks drives the\r logistic loss to zero, and prove bounds on the rate\r
  of convergence. Our analysis applies for smoothed\r approximations to the ReLU,
  such as Swish and the\r Huberized ReLU, proposed in previous applied\r work. We
  provide two sufficient conditions for\r convergence.  The first is simply a bound
  on the\r loss at initialization.  The second is a data\r separation condition used
  in prior analyses."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chatterji21a
month: 0
tex_title: "When does gradient descent with logistic loss\r interpolate using deep
  networks with smoothed ReLU\r activations?"
firstpage: 927
lastpage: 1027
page: 927-1027
order: 927
cycles: false
bibtex_author: Chatterji, Niladri S and Long, Philip M and Bartlett, Peter
author:
- given: Niladri S
  family: Chatterji
- given: Philip M
  family: Long
- given: Peter
  family: Bartlett
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/chatterji21a/chatterji21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
