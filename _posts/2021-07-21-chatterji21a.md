---
title: When does gradient descent with logistic loss interpolate using deep networks
  with smoothed ReLU activations?
abstract: We establish conditions under which gradient descent applied to fixed-width
  deep networks drives the logistic loss to zero, and prove bounds on the rate of
  convergence. Our analysis applies for smoothed approximations to the ReLU, such
  as Swish and the Huberized ReLU, proposed in previous applied work. We provide two
  sufficient conditions for convergence.  The first is simply a bound on the loss
  at initialization.  The second is a data separation condition used in prior analyses.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chatterji21a
month: 0
tex_title: When does gradient descent with logistic loss interpolate using deep networks
  with smoothed ReLU activations?
firstpage: 927
lastpage: 1027
page: 927-1027
order: 927
cycles: false
bibtex_author: Chatterji, Niladri S. and Long, Philip M. and Bartlett, Peter
author:
- given: Niladri S.
  family: Chatterji
- given: Philip M.
  family: Long
- given: Peter
  family: Bartlett
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/chatterji21a/chatterji21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
