---
title: Optimal Dynamic Regret in Exp-Concave Online Learning
abstract: We consider the problem of the Zinkevich (2003)-style dynamic regret minimization
  in online learning with \emph{exp-concave} losses. We show that whenever improper
  learning is allowed, a Strongly Adaptive online learner achieves the dynamic regret
  of $\tilde O^*(n^{1/3}C_n^{2/3} \vee 1)$ where $C_n$ is the \emph{total variation}
  (a.k.a. \emph{path length}) of the an arbitrary sequence of comparators that may
  not be known to the learner ahead of time. Achieving this rate was highly nontrivial
  even for square losses in 1D where the best known upper bound was $O(\sqrt{nC_n}
  \vee \log n)$ (Yuan and Lamperski, 2019). Our new proof techniques make elegant
  use of the intricate structures of the primal and dual variables imposed by the
  KKT conditions and could be of independent interest. Finally, we apply our results
  to the classical statistical problem of \emph{locally adaptive non-parametric regression}
  (Mammen, 1991; Donoho and Johnstone, 1998) and obtain a stronger and more flexible
  algorithm that do not require any statistical assumptions or any hyperparameter
  tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: baby21a
month: 0
tex_title: Optimal Dynamic Regret in Exp-Concave Online Learning
firstpage: 359
lastpage: 409
page: 359-409
order: 359
cycles: false
bibtex_author: Baby, Dheeraj and Wang, Yu-Xiang
author:
- given: Dheeraj
  family: Baby
- given: Yu-Xiang
  family: Wang
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/baby21a/baby21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
