---
title: Concentration of Non-Isotropic Random Tensors with Applications to Learning
  and Empirical Risk Minimization
abstract: 'Dimension is an inherent bottleneck to some modern learning tasks, where
  optimization methods suffer from the size of the data. In this paper, we study non-isotropic
  distributions of data and develop tools that aim at reducing these dimensional costs
  by a dependency on an effective dimension rather than the ambient one.  Based on
  non-asymptotic estimates of the metric entropy of ellipsoids -that prove to generalize
  to infinite dimensions- and on a chaining argument, our uniform concentration bounds
  involve an effective dimension instead of the global dimension, improving over existing
  results.  We show the importance of taking advantage of non-isotropic properties
  in learning problems with the following applications: i) we improve state-of-the-art
  results in statistical preconditioning for communication-efficient distributed optimization,
  ii) we introduce a non-isotropic randomized smoothing for non-smooth optimization.
  Both applications cover a class of functions that encompasses empirical risk minization
  (ERM) for linear models.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: even21a
month: 0
tex_title: Concentration of Non-Isotropic Random Tensors with Applications to Learning
  and Empirical Risk Minimization
firstpage: 1847
lastpage: 1886
page: 1847-1886
order: 1847
cycles: false
bibtex_author: Even, Mathieu and Massoulie, Laurent
author:
- given: Mathieu
  family: Even
- given: Laurent
  family: Massoulie
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/even21a/even21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
