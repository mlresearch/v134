---
title: "Learning from Censored and Dependent Data: The case\r of Linear Dynamics"
abstract: "Observations from dynamical systems often exhibit\r irregularities, such
  as censoring, where values are\r recorded only if they fall within a certain\r range.
  Censoring is ubiquitous in practice, due to\r saturating sensors, limit-of-detection
  effects,\r image frame effects, and combined with the temporal\r dependencies within
  the data, makes the task of\r system identification particularly challenging.  In\r
  light of recent developments on learning linear\r dynamical systems (LDSs), and
  on censored statistics\r with independent data, we revisit the decades-old\r problem
  of learning an LDS, from censored\r observations (Lee and Maddala (1985), Zeger
  and\r Brookmeyer (1986)). Here, the learner observes the\r state x_t \\in R^d if
  and only if x_t belongs to some\r set S_t _x0012_\\in R^d. We develop the first\r
  computationally and statistically efficient\r algorithm for learning the system,
  assuming only\r oracle to the sets St. Our algorithm, Stochastic\r Online Newton
  with Switching Gradients, is a novel\r second-order method that builds on the Online
  Newton\r Step (ONS) of Hazan et al. (2007). Our\r Switching-Gradient scheme does
  not always use\r (stochastic) gradients of the function we want to\r optimize, which
  we call censor-aware\r function. Instead, in each iteration, it performs a\r simple
  test to decide whether to use the\r censor-aware, or another censor-oblivious function,\r
  for getting a stochastic gradient.  In our analysis,\r we consider a “generic” Online
  Newton method, which\r uses arbitrary vectors instead of gradients, and we\r prove
  an error-bound for it. This can be used to\r appropriately design these vectors,
  Leading to our\r Switching-Gradient scheme. This framework\r significantly deviates
  from the recent long line of\r works on censored statistics (e.g, Daskalakis et\r
  al. (2018); Kontonis et al. (2019); Daskalakis et\r al. (2019), which apply Stochastic
  Gradient Descent\r (SGD), and their analysis reduces to establishing\r conditions
  for off-the-shelf SGD-bounds. Our\r approach enables to relax these conditions,
  and\r gives rise to phenomena that might appear\r counterintuitive, given the previous\r
  works. Specifically, our method makes progress even\r when the current “survival
  probability” is\r exponentially small. We believe that our analysis\r framework
  will have applications in more settings\r where the data are subject to censoring."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: plevrakis21a
month: 0
tex_title: "Learning from Censored and Dependent Data: The case\r of Linear Dynamics"
firstpage: 3771
lastpage: 3787
page: 3771-3787
order: 3771
cycles: false
bibtex_author: Plevrakis, Orestis
author:
- given: Orestis
  family: Plevrakis
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/plevrakis21a/plevrakis21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
