---
title: "Multiplayer Bandit Learning, from Competition to\r Cooperation"
abstract: "The stochastic multi-armed bandit model captures the\r tradeoff between
  exploration and exploitation. We\r study the effects of competition and cooperation
  on\r this tradeoff. Suppose there are two arms, one\r predictable and one risky,
  and two players, Alice\r and Bob. In every round, each player pulls an arm,\r receives
  the resulting reward, and observes the\r choice of the other player but not their\r
  reward. Alice’s utility is $\\Gamma_A + \\lambda\r \\Gamma_B$ (and similarly for
  Bob), where $\\Gamma_A$\r is Alice’s total reward and $\\lambda \\in [-1, 1]$ is\r
  a cooperation parameter. At $\\lambda = -1$ the\r players are competing in a zero-sum
  game, at\r $\\lambda = 1$, their interests are aligned, and at\r $\\lambda = 0$,
  they are neutral: each player’s\r utility is their own reward. The model is related
  to\r the economics literature on strategic\r experimentation, where usually players
  observe each\r other’s rewards.  Suppose the predictable arm has\r success probability
  $p$ and the risky arm has prior\r $\\mu$. If the discount factor is $\\beta$, then
  the\r value of $p$ where a single player is indifferent\r between the arms is the
  Gittins index $g =\r g(\\mu,\\beta) > m$, where $m$ is the mean of the\r risky arm.
  \ Our first result answers, in this\r setting, a fundamental question posed by\r
  Rothschild \\cite{rotschild}. We show that competing\r and neutral players eventually
  settle on the same\r arm (even though it may not be the best arm) in\r every Nash
  equilibrium, while this can fail for\r players with aligned interests.  Moreover,
  we show\r that \\emph{competing players} explore \\emph{less}\r than a single player:
  there is $p^* \\in (m, g)$ so\r that for all $p > p^*$, the players stay at the\r
  predictable arm. However, the players are not\r myopic: they still explore for some
  $p > m$. On the\r other hand, \\emph{cooperating players} (with\r $\\lambda =1$)
  explore \\emph{more} than a single\r player. We also show that \\emph{neutral players}\r
  learn from each other, receiving strictly higher\r total rewards than they would
  playing alone, for all\r $ p\\in (p^*, g)$, where $p^*$ is the threshold above\r
  which competing players do not explore."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: branzei21a
month: 0
tex_title: "Multiplayer Bandit Learning, from Competition to\r Cooperation"
firstpage: 679
lastpage: 723
page: 679-723
order: 679
cycles: false
bibtex_author: Branzei, Simina and Peres, Yuval
author:
- given: Simina
  family: Branzei
- given: Yuval
  family: Peres
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/branzei21a/branzei21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
