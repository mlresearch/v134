---
title: "Size and Depth Separation in Approximating Benign\r Functions with Neural
  Networks"
abstract: "When studying the expressive power of neural\r networks, a main challenge
  is to understand how the\r size and depth of the network affect its ability to\r
  approximate real functions.  However, not all\r functions are interesting from a
  practical\r viewpoint: functions of interest usually have a\r polynomially-bounded
  Lipschitz constant, and can be\r computed efficiently. We call functions that satisfy\r
  these conditions “benign\", and explore the benefits\r of size and depth for approximation
  of benign\r functions with ReLU networks.  As we show, this\r problem is more challenging
  than the corresponding\r problem for non-benign functions.  We give\r complexity-theoretic
  barriers to showing\r depth-lower-bounds: Proving existence of a benign\r function
  that cannot be approximated by\r polynomial-sized networks of depth $4$ would settle\r
  longstanding open problems in computational\r complexity. It implies that beyond
  depth $4$ there\r is a barrier to showing depth-separation for benign\r functions,
  even between networks of constant depth\r and networks of nonconstant depth.  We
  also study\r size-separation, namely, whether there are benign\r functions that
  can be approximated with networks of\r size $O(s(d))$, but not with networks of
  size\r $O(s’(d))$. We show a complexity-theoretic barrier\r to proving such results
  beyond size $O(d\\log^2(d))$,\r but also show an explicit benign function, that
  can\r be approximated with networks of size $O(d)$ and not\r with networks of size
  $o(d/\\log d)$. For\r approximation in the $L_\\infty$ sense we achieve\r such separation
  already between size $O(d)$ and size\r $o(d)$.  Moreover, we show superpolynomial
  size\r lower bounds and barriers to such lower bounds,\r depending on the assumptions
  on the function.  Our\r size-separation results rely on an analysis of size\r lower
  bounds for Boolean functions, which is of\r independent interest: We show linear
  size lower\r bounds for computing explicit Boolean functions\r (such as set disjointness)
  with neural networks and\r threshold circuits."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vardi21a
month: 0
tex_title: "Size and Depth Separation in Approximating Benign\r Functions with Neural
  Networks"
firstpage: 4195
lastpage: 4223
page: 4195-4223
order: 4195
cycles: false
bibtex_author: Vardi, Gal and Reichman, Daniel and Pitassi, Toniann and Shamir, Ohad
author:
- given: Gal
  family: Vardi
- given: Daniel
  family: Reichman
- given: Toniann
  family: Pitassi
- given: Ohad
  family: Shamir
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/vardi21a/vardi21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
