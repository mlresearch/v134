---
title: Online Learning with Simple Predictors and a Combinatorial Characterization
  of Minimax in 0/1 Games
abstract: 'Which classes can be learned properly in the online model? — that is, by
  an algorithm that on each round uses a predictor from the concept class. While there
  are simple and natural cases where improper learning is useful and even necessary,
  it is natural to ask how complex must the improper predictors be in such cases.  Can
  one always achieve nearly optimal mistake/regret bounds using "simple" predictors?  In
  this work, we give a complete characterization of when this is possible, thus settling
  an open problem which has been studied since the pioneering works of Angluin (1987)
  and Littlestone (1988).  More precisely, given any concept class C and any hypothesis
  class H, we provide nearly tight bounds (up to a log factor) on the optimal mistake
  bounds for online learning C using predictors from H.  Our bound yields an exponential
  improvement over the previously best known bound by Chase and Freitag (2020).  As
  applications, we give constructive proofs showing that (i) in the realizable setting,
  a near-optimal mistake bound (up to a constant factor) can be attained by a sparse
  majority-vote of proper predictors, and (ii) in the agnostic setting, a near-optimal
  regret bound (up to a log factor) can be attained by a randomized proper algorithm.  The
  latter was proven non-constructively by Rakhlin, Sridharan, and Tewari (2015).  It
  was also achieved by constructive but improper algorithms proposed by Ben-David,
  Pal, and Shalev-Shwartz (2009) and Rakhlin, Shamir, and Sridharan (2012).  A technical
  ingredient of our proof which may be of independent interest is a generalization
  of the celebrated Minimax Theorem (von Neumann, 1928) for binary zero-sum games
  with arbitrary action-sets: a simple game which fails to satisfy Minimax is "Guess
  the Larger Number".  In this game, each player picks a natural number and the player
  who picked the larger number wins.  Equivalently, the payoff matrix of this game
  is infinite triangular.  We show that this is the only obstruction: if the payoff
  matrix does not contain triangular submatrices of unbounded sizes then the Minimax
  theorem is satisfied.  This generalizes von Neumann’s Minimax Theorem by removing
  requirements of finiteness (or compactness) of the action-sets, and moreover it
  captures precisely the types of games of interest in online learning: namely, Littlestone
  games.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hanneke21a
month: 0
tex_title: Online Learning with Simple Predictors and a Combinatorial Characterization
  of Minimax in 0/1 Games
firstpage: 2289
lastpage: 2314
page: 2289-2314
order: 2289
cycles: false
bibtex_author: Hanneke, Steve and Livni, Roi and Moran, Shay
author:
- given: Steve
  family: Hanneke
- given: Roi
  family: Livni
- given: Shay
  family: Moran
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/hanneke21a/hanneke21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
