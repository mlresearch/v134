---
title: Near Optimal Distributed Learning of Halfspaces with Two Parties
abstract: '{\em Distributed learning} protocols are designed to train on distributed data without gathering it all on a single centralized machine, 
thus contributing to the efficiency of the system and enhancing its privacy. 
We study a central problem in distributed learning, called {\it distributed learning of halfspaces}: 
let $U \subseteq \mathbb{R}^d$ be a known domain of size $n$ and let $h:\mathbb{R}^d\to \mathbb{R}$ be an unknown target affine function.\footnote{In practice, the domain $U$ is defined implicitly by the representation of $d$-dimensional vectors which is used in the protocol.}
A set of {\it examples} $\{(u,b)\}$ is distributed between several parties, where~$u \in U$ is a point and $b = \mathsf{sign}(h(u)) \in \{\pm 1\}$ is its label. 
The parties' goal is to agree on a classifier~$f: U\to\{\pm 1\}$ such that~$f(u)=b$ for every input example~$(u,b)$.

We design a protocol for the distributed halfspace learning problem in the two-party setting, communicating only $\tilde O(d\log n)$ bits.  
To this end, we introduce a new tool called {\em halfspace containers}, that is closely related to {\em bracketing numbers} in statistics and to {\it hyperplane cuttings} in discrete geometry, 
and allows for a compressed approximate representation of every halfspace.
We complement our upper bound result by an almost matching $\tilde \Omega(d\log n)$ lower bound on the communication complexity of any such protocol

Since the distributed halfspace learning problem is closely related to the {\em convex set disjointness} problem in communication complexity 
and the problem of {\em distributed linear programming} in distributed optimization,
we also derive upper and lower bounds of $\tilde O(d^2\log n)$ and~$\tilde{\Omega}(d\log n)$ on the communication complexity of both of these basic problems.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: braverman21a
month: 0
tex_title: Near Optimal Distributed Learning of Halfspaces with Two Parties
firstpage: 724
lastpage: 758
page: 724-758
order: 724
cycles: false
bibtex_author: Braverman, Mark and Kol, Gillat and Moran, Shay and Saxena, Raghuvansh
  R.
author:
- given: Mark
  family: Braverman
- given: Gillat
  family: Kol
- given: Shay
  family: Moran
- given: Raghuvansh R.
  family: Saxena
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/braverman21a/braverman21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
