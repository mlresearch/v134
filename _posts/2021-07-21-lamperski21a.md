---
title: Projected Stochastic Gradient Langevin Algorithms for Constrained Sampling
  and Non-Convex Learning
abstract: Langevin algorithms are gradient descent methods with additive noise. They
  have been used for decades in Markov Chain Monte Carlo (MCMC) sampling, optimization,
  and learning. Their convergence properties for unconstrained non-convex optimization
  and learning problems have been studied widely in the last few years. Other work
  has examined projected Langevin algorithms for sampling from log-concave distributions
  restricted to convex compact sets. For learning and optimization, log-concave distributions
  correspond to convex losses. In this paper, we analyze the case of non-convex losses
  with compact convex constraint sets and IID external data variables. We term the
  resulting method the projected stochastic gradient Langevin algorithm (PSGLA). We
  show the algorithm achieves a deviation of $O(T^{-1/4} (log T )^{1/2} )$ from its
  target distribution in 1-Wasserstein distance. For optimization and learning, we
  show that the algorithm achieves $\epsilon$-suboptimal solutions, on average, provided
  that it is run for a time that is polynomial in $\epsilon$ and slightly super-exponential
  in the problem dimension.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lamperski21a
month: 0
tex_title: Projected Stochastic Gradient Langevin Algorithms for Constrained Sampling
  and Non-Convex Learning
firstpage: 2891
lastpage: 2937
page: 2891-2937
order: 2891
cycles: false
bibtex_author: Lamperski, Andrew
author:
- given: Andrew
  family: Lamperski
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/lamperski21a/lamperski21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
