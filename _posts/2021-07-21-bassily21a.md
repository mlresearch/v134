---
title: Non-Euclidean Differentially Private Stochastic Convex Optimization
abstract: Differentially private (DP) stochastic convex optimization (SCO) is a fundamental
  problem, where the goal is to approximately minimize the population risk with respect
  to a convex loss function, given a dataset of i.i.d. samples from a distribution,
  while satisfying differential privacy with respect to the dataset. Most of the existing
  works in the literature of private convex optimization focus on the Euclidean (i.e.,
  $\ell_2$) setting, where the loss is assumed to be Lipschitz (and possibly smooth)
  w.r.t. the $\ell_2$ norm over a constraint set with bounded $\ell_2$ diameter. Algorithms
  based on noisy stochastic gradient descent (SGD) are known to attain the optimal
  excess risk in this setting. In this work, we conduct a systematic study of DP-SCO
  for $\ell_p$-setups. For $p=1$, under a standard smoothness assumption, we give
  a new algorithm with nearly optimal excess risk. This result also extends to general
  polyhedral norms and feasible sets. For $p\in(1, 2)$, we give two new algorithms,
  whose central building block is a novel privacy mechanism, which generalizes the
  Gaussian mechanism. Moreover, we establish a lower bound on the excess risk for
  this range of $p$, showing a necessary dependence on $\sqrt{d}$, where $d$ is the
  dimension of the space. Our lower bound implies a sudden transition of the excess
  risk at $p=1$, where the dependence on $d$ changes from logarithmic to polynomial,
  resolving an open question in prior work \citep{TTZ15a}. For $p\in (2, \infty)$,
  noisy SGD attains optimal excess risk in the low-dimensional regime; in particular,
  this proves the optimality of noisy SGD for $p=\infty$. Our work draws upon concepts
  from the geometry of normed spaces, such as the notions of regularity, uniform convexity,
  and uniform smoothness.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bassily21a
month: 0
tex_title: Non-Euclidean Differentially Private Stochastic Convex Optimization
firstpage: 474
lastpage: 499
page: 474-499
order: 474
cycles: false
bibtex_author: Bassily, Raef and Guzman, Cristobal and Nandi, Anupama
author:
- given: Raef
  family: Bassily
- given: Cristobal
  family: Guzman
- given: Anupama
  family: Nandi
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/bassily21a/bassily21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
