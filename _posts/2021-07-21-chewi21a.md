---
title: Optimal dimension dependence of the Metropolis-Adjusted Langevin Algorithm
abstract: Conventional wisdom in the sampling literature, backed by a popular diffusion
  scaling limit, suggests that the mixing time of the Metropolis-Adjusted Langevin
  Algorithm (MALA) scales as O(d^{1/3}), where d is the dimension. However, the diffusion
  scaling limit requires stringent assumptions on the target distribution and is asymptotic
  in nature. In contrast, the best known non-asymptotic mixing time bound for MALA
  on the class of log-smooth and strongly log-concave distributions is O(d). In this
  work, we establish that the mixing time of MALA on this class of target distributions
  is \tilde\Theta(d^{1/2}) under a warm start.  Our upper bound proof introduces a
  new technique based on a projection characterization of the Metropolis adjustment
  which reduces the study of MALA to the well-studied discretization analysis of the
  Langevin SDE and bypasses direct computation of the acceptance probability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chewi21a
month: 0
tex_title: Optimal dimension dependence of the Metropolis-Adjusted Langevin Algorithm
firstpage: 1260
lastpage: 1300
page: 1260-1300
order: 1260
cycles: false
bibtex_author: Chewi, Sinho and Lu, Chen and Ahn, Kwangjun and Cheng, Xiang and Gouic,
  Thibaut Le and Rigollet, Philippe
author:
- given: Sinho
  family: Chewi
- given: Chen
  family: Lu
- given: Kwangjun
  family: Ahn
- given: Xiang
  family: Cheng
- given: Thibaut Le
  family: Gouic
- given: Philippe
  family: Rigollet
date: 2021-07-21
address:
container-title: Proceedings of Thirty Fourth Conference on Learning Theory
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/chewi21a/chewi21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
