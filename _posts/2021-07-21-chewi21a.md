---
title: "Optimal dimension dependence of the\r Metropolis-Adjusted Langevin Algorithm"
abstract: "Conventional wisdom in the sampling literature,\r backed by a popular diffusion
  scaling limit,\r suggests that the mixing time of the\r Metropolis-Adjusted Langevin
  Algorithm (MALA) scales\r as O(d^{1/3}), where d is the dimension. However,\r the
  diffusion scaling limit requires stringent\r assumptions on the target distribution
  and is\r asymptotic in nature. In contrast, the best known\r non-asymptotic mixing
  time bound for MALA on the\r class of log-smooth and strongly log-concave\r distributions
  is O(d). In this work, we establish\r that the mixing time of MALA on this class
  of target\r distributions is \\tilde\\Theta(d^{1/2}) under a warm\r start.  Our
  upper bound proof introduces a new\r technique based on a projection characterization
  of\r the Metropolis adjustment which reduces the study of\r MALA to the well-studied
  discretization analysis of\r the Langevin SDE and bypasses direct computation of\r
  the acceptance probability."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chewi21a
month: 0
tex_title: "Optimal dimension dependence of the\r Metropolis-Adjusted Langevin Algorithm"
firstpage: 1260
lastpage: 1300
page: 1260-1300
order: 1260
cycles: false
bibtex_author: Chewi, Sinho and Lu, Chen and Ahn, Kwangjun and Cheng, Xiang and Gouic,
  Thibaut Le and Rigollet, Philippe
author:
- given: Sinho
  family: Chewi
- given: Chen
  family: Lu
- given: Kwangjun
  family: Ahn
- given: Xiang
  family: Cheng
- given: Thibaut Le
  family: Gouic
- given: Philippe
  family: Rigollet
date: 2021-07-21
address:
container-title: "Proceedings of Thirty Fourth Conference on Learning\r Theory"
volume: '134'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 21
pdf: http://proceedings.mlr.press/v134/chewi21a/chewi21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
